{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report for the @WeRateDogs Project\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction:\n",
    "My Data Wrangling Process covered the 3 following Steps in this Order:\n",
    "- Gathering Data\n",
    "- Assessing Data\n",
    "- Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data:\n",
    "For this Part, I had to acquire Data from 3 different Sources in 3 Pandas dataframes:<br>\n",
    "1) The Twitter Archive was imported from a <i>CSV</i> File, 'twitter-archive-enhanced.csv'<br>\n",
    "2) The Image Prediction File 'image-predictions.tsv', was programmatically downloaded with the <i>requests</i> python Library<br>\n",
    "3) The Twitter Archive was queryied from the Twitter API, with the \"tweepy\" Wrapper and I saved each obtained JSON Response, as a JSON String in a TextFile called 'tweet_json.txt'.<br>I finally read this textfile line by line, adding each read JSON Object in a list of Dictionnaries and finally imported this List in a Dataframe.<br>Note: I saved each Tweet Id that could not be queryied via \"tweepy\", in a \"tweetid_errors.txt\" Logfile for later Verification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data:\n",
    "I assessed visually and programmatically each Dataframe and documented Quality and Tidiness Issues found along the way, building up a \"Cleaning Plan\".<br>\n",
    "I also included the Key Points from the Project Details, specifying necessary Conditions about the Records. I came up with the following List of Issues(11 quality issues and 5 tidiness issues):<br> \n",
    "\n",
    "#### Quality Issues in the Archive Dataframe:\n",
    "\n",
    "- timestamp should be a datetime Object.\n",
    "- some tweets are obsolete and must be removed (could not be fetched from the tweepy API)\n",
    "- some tweets are retweets (181 non null 'retweeted_status_id') and must be removed\n",
    "- there are missing values in the 'expended_urls' Column.\n",
    "- The ratings were parsed from the Text with a Method that led to accuracy issues\n",
    "- the data type for the rating numerator should be a float, not an integer\n",
    "- Some Ratings voluntary have inacurrate ('fantasy') Values. I decide to keep only tweets that have a rating between 0 and 14 over 10, (there is enough fantasy in this already!)\n",
    "- Incorrect parsed dog names \"a\"\n",
    "- Some tweets do not have an image. All the Records from the Archive (2356 entries) should also be in the Prediction Dataframe(2075 entries).\n",
    "\n",
    "#### Quality Issue in the tweepy Dataframe:\n",
    "- There are Retweets and Quotes in the tweepy Dataframe. I need to make sure that these are consistent with the Retweets/Quotes from the Archive.\n",
    "\n",
    "#### Quality Issue in the Prediction DataFrame:\n",
    "- Some tweets do not represent dogs.\n",
    "\n",
    "#### Tidiness Issues in the Archive\n",
    "- The rating for each record is kept in two Columns 'rating_numerator' and 'rating_denominator' \n",
    "- The values corresponding to the \"dog stage\" Variable is contained in four columns/variables: \"doggo\", \"pupper\", \"puppo\", \"fluffer\"\n",
    "\n",
    "#### Tidiness Issue in the tweepy Dataframe\n",
    "- The favorite and retweet counts fo each tweet should be part of the Archive Dataframe.\n",
    "\n",
    "#### Tidiness Issues in the Prediction Dataframe:\n",
    "- There are several possible values for the predicted breed. We want to keep the most plausible one\n",
    "- The \"best\" predicted breed \"column\" should be added to our Master Dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data:\n",
    "\n",
    "\n",
    "For this Part, I first made a copy of each Dataframe to clean on and then proceeded according to the following Plan:<br>\n",
    "\n",
    "I) I first adressed the Issues to make the three Dataframes \"match\" together. It means that I want to keep the tweets in the Archive that were not deleted and also are in the Tweepy Dataframe. Also each tweet from the Archive and Tweepy Dataframes, should have a Picture and be in the Prediction Dataframe <br>\n",
    "II) I then addressed the Issues where Data was missing, whenever possible<br>\n",
    "III) I then addressed the Tidiness Issues, whenever possible<br>\n",
    "IV) I finally worked on the remaining Quality Issues<br><br>\n",
    "I fixed each Issue in following the three known Steps:<br>\n",
    "1) Define<br>\n",
    "2) Code<br>\n",
    "3) Test<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Challenges encountered while cleaning the data and how I solved them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) To merge specific Columns from the Tweepy and Prediction Dataframe with the Archive, I used the Pandas merge Function, joining the dataframes on the Tweet Id like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive = pd.merge(df_archive, df_tweepy, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) To parse the \"dog stage\" from the Text Columns, I wrote the following Function that I mapped on each Row of the Archive Dataframe like so:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_stages= ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "def parse_dog_stages(text):\n",
    "    stages = ' , '.join([stage for stage in dogs_stages if stage in text])\n",
    "    if ',' in stages:\n",
    "        return 'Multiple'\n",
    "    if stages:\n",
    "        return stages.capitalize()\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) To keep only one Breed Variable from the Prediction Dataframe, I wrote a Function calculating the breed of each Record, with the highest Prediction Confidence, when a Dog was recognized in the Picture. Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_predicted_breed(row):\n",
    "    breeds = []\n",
    "    predictions = []\n",
    "    if (row['p1_dog']):\n",
    "        breeds.append(row['p1'])\n",
    "        predictions.append(row['p1_conf'])\n",
    "    if (row['p2_dog']):\n",
    "        breeds.append(row['p2'])\n",
    "        predictions.append(row['p2_conf'])\n",
    "    if (row['p3_dog']):\n",
    "        breeds.append(row['p3'])\n",
    "        predictions.append(row['p3_conf'])\n",
    "    #index_max = predictions.index(max(predictions))\n",
    "    return breeds[0] # since p1_conf > p2_conf > p3_conf and the order in which we append\n",
    "    # the values, the breed for the highest confidence is always at index 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) To parse the Ratings from the text Column and keep the Result in a Dataframe I used the following regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res = df_archive_clean.text.str.extract(r'(?P<num>\\d{1,2}\\.?\\d*)/10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data\n",
    "\n",
    "I finally stored the cleaned master Dataset in a <i>CSV</i> File 'twitter_archive_master.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
